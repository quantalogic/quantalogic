functions:
  generate_outline:
    type: embedded
    code: |-
      @Nodes.llm_node(system_prompt='You are a creative writer skilled at generating stories.', prompt_template='Create a story outline for a {genre} story with {num_chapters} chapters.', output='outline', **DEFAULT_LLM_PARAMS)
      def generate_outline(genre, num_chapters):
          """Generate a story outline based on genre and number of chapters."""
          return {}
    module: null
    function: null
  generate_chapter:
    type: embedded
    code: |-
      @Nodes.llm_node(system_prompt='You are a creative writer.', prompt_template='Write chapter {chapter_num} for this story outline: {outline}. Style: {style}.', output='chapter', **DEFAULT_LLM_PARAMS)
      def generate_chapter(outline, chapter_num, style):
          """Generate a single chapter based on the outline."""
          return {}
    module: null
    function: null
  update_progress:
    type: embedded
    code: "@Nodes.define(output='updated_context')\nasync def update_progress(**context):\n    \"\"\"Update the progress of\
      \ chapter generation.\n    \n    Takes the entire context dictionary and handles missing keys gracefully.\n    \"\"\"\
      \n    chapters = context.get('chapters', [])\n    completed_chapters = context.get('completed_chapters', 0)\n    chapter\
      \ = context.get('chapter', {})\n    updated_chapters = chapters.copy()\n    updated_chapters.append(chapter)\n    updated_context\
      \ = {**context, 'chapters': updated_chapters, 'completed_chapters': completed_chapters + 1}\n    return updated_context"
    module: null
    function: null
  check_if_complete:
    type: embedded
    code: "@Nodes.define(output='continue_generating')\nasync def check_if_complete(completed_chapters=0, num_chapters=0,\
      \ **kwargs):\n    \"\"\"Check if all chapters have been generated.\n    \n    Args:\n        completed_chapters: Number\
      \ of chapters completed so far\n        num_chapters: Total number of chapters to generate\n        kwargs: Additional\
      \ context parameters\n        \n    Returns:\n        bool: True if we should continue generating chapters, False otherwise\n\
      \    \"\"\"\n    return completed_chapters < num_chapters"
    module: null
    function: null
nodes:
  generate_outline:
    function: null
    sub_workflow: null
    llm_config:
      model: gemini/gemini-2.0-flash
      system_prompt: You are a creative writer skilled at generating stories.
      prompt_template: Create a story outline for a {genre} story with {num_chapters} chapters.
      temperature: 0.7
      max_tokens: 1000
      top_p: 1.0
      presence_penalty: 0.0
      frequency_penalty: 0.0
      stop: null
      response_model: null
      api_key: null
    output: outline
    retries: 3
    delay: 1.0
    timeout: null
    parallel: false
  generate_chapter:
    function: null
    sub_workflow: null
    llm_config:
      model: gemini/gemini-2.0-flash
      system_prompt: You are a creative writer.
      prompt_template: 'Write chapter {chapter_num} for this story outline: {outline}. Style: {style}.'
      temperature: 0.7
      max_tokens: 1000
      top_p: 1.0
      presence_penalty: 0.0
      frequency_penalty: 0.0
      stop: null
      response_model: null
      api_key: null
    output: chapter
    retries: 3
    delay: 1.0
    timeout: null
    parallel: false
  update_progress:
    function: update_progress
    sub_workflow: null
    llm_config: null
    output: updated_context
    retries: 3
    delay: 1.0
    timeout: null
    parallel: false
  check_if_complete:
    function: check_if_complete
    sub_workflow: null
    llm_config: null
    output: continue_generating
    retries: 3
    delay: 1.0
    timeout: null
    parallel: false
workflow:
  start: generate_outline
  transitions:
  - from_node: generate_outline
    to_node: generate_chapter
    condition: null
  - from_node: generate_chapter
    to_node: update_progress
    condition: null
  - from_node: update_progress
    to_node: check_if_complete
    condition: null
  - from_node: check_if_complete
    to_node: generate_chapter
    condition: 'lambda ctx: ctx.get(''continue_generating'', False)'
  - from_node: generate_chapter
    to_node: update_progress
    condition: null
  - from_node: update_progress
    to_node: check_if_complete
    condition: null
observers: []
